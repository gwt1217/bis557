x_train <- array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), img_rows, img_cols, 1))
input_shape <- c(img_rows, img_cols, 1)
# Transform RGB values into [0,1] range
x_train <- x_train / 255
x_test <- x_test / 255
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
# Define model
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3,3), activation = 'relu',
input_shape = input_shape) %>%
layer_conv_2d(filters = 64, kernel_size = c(3,3), activation = 'relu') %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = num_classes, activation = 'softmax')
# Compile model
model %>% compile(
loss = loss_categorical_crossentropy,
optimizer = optimizer_adadelta(),
metrics = c('accuracy')
)
# Train model
model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
validation_split = 0.2
)
scores <- model %>% evaluate(
x_test, y_test, verbose = 0
)
# The final result
cat('Test loss:', scores[[1]], '\n')
cat('Test accuracy:', scores[[2]], '\n')
install.package(R.matlab)
install.package("R.matlab")
install.packages("R.matlab")
library(R.matlab)
emnist <- readMat('/Users/wentinggao/Downloads/emnist-byclass.mat')
save(emnist, file='emnist.RData')
load(file='/Users/wentinggao/Downloads/emnist.RData')
x_train <- emnist$dataset[[1]][[1]]
X_train <- list()
for (i in 1:nrow(x_train)) { #unflatten
X_train[[i]] <- matrix(x_train[i,], nrow=28, ncol=28)
}
X_train <- X_train/255 #change to 0-1
Y_train <- emnist$dataset[[1]][[2]]
x_train <- emnist$dataset[[1]][[1]]
Y_train <- to_categorical(Y_train, 62)
emnist_train <- emnist$dataset[[1]][[1]]
emnist_train <- emnist$dataset[[1]][[2]]
library(keras)
x_train = as.matrix(emnist_train[,-1])
y_train_cat = emnist_train[, 1] - 1
y_train = as.matrix(y_train_cat)
x_valid = as.matrix(emnist_valid[,-1])
emnist_train <- emnist$dataset[[1]][[1]]
emnist_test <- emnist$dataset[[1]][[2]]
x_train = as.matrix(emnist_train[,-1])
y_train_cat = emnist_train[, 1] - 1
y_train = as.matrix(y_train_cat)
x_test = as.matrix(emnist_test[,-1])
y_test_cat = emnist_test[, 1] - 1
y_test = as.matrix(y_test_cat)
# Data Preparation
batch_size = 128
num_classes = 26
# Input image dimensions
img_rows = 28
img_cols = 28
# Redefine  dimension of train/test inputs
x_train = array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
Y_train <- data.frame(train_id="train", class=y_train, class_name=letters[as.numeric(y_train)])
class(emnist_train)
dim(emnist_train)
head(colnames(emnist_train))
x_train = array_reshape(x_train, c(nrow(x_train), img_rows, img_cols, 1))
y_test = as.matrix(emnist_test$`1`)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
data("emnist_test")
library(keras)
data("emnist_test")
emnist
emnist_train <- emnist$dataset[[1]][[1]]
emnist_test <- emnist$dataset[[1]][[2]]
X_train <- list()
for (i in 1:nrow(x_train)) { #unflatten
X_train[[i]] <- matrix(x_train[i,], nrow=28, ncol=28)
}
# casl_nn_init_weights API
# Initiate weight matrics for a dense neural network.
#
# Args:
#     layer_sizes: A vector(numeric) showing the size of each layer
#
# Returns:
#     A list containing initialized weights and biases.
casl_nn_init_weights <- function(layer_sizes){
len<- length(layer_sizes) - 1
weights <- vector("list", len)
for (i in seq_len(len)){
w <- matrix(rnorm(layer_sizes[i] * layer_sizes[i + 1]),
ncol = layer_sizes[i],
nrow = layer_sizes[i + 1])
weights[[i]] <- list(w=w,
b=rnorm(layer_sizes[i + 1]))
}
weights
}
weight <- casl_nn_init_weights(c(1, 25, 1))
# casl_util_ReLU API
# Apply a rectified linear unit (ReLU) to a vector / matrix.
#
# Args:
#     vec: A vector(numeric) or matrix(numeric).
#
# Returns:
#     The original input with negative values truncated to 0.
casl_util_ReLU <- function(vec){
vec[vec < 0] <- 0
vec
}
# casl_util_ReLU_derivative API
# Apply derivative of the rectified linear unit (ReLU).
#
# Args:
#     vec: A vector(numeric) or matrix(numeric).
#
# Returns:
#     Returned with positive values to 1 and negative values to 0.
casl_util_ReLU_derivative <- function(vec){
res <- vec * 0
res[vec > 0] <- 1
res
}
# casl_util_mad_p API
# Derivative of the mean absolute deviance (MAD).
#
# Args:
#     y: A vector(numeric) of responses.
#     pred: A vector(numeric) of predicted responses.
#
# Returns:
#     Returned current derivative MAD
casl_util_mad_p <- function(y, pred){
dev <- c()
for(i in seq_along(pred)){
if(pred[i] >= y[i]) dev[i] <- 1
else dev[i] <- -1
}
dev
}
# casl_nn_forward_prop API
# Apply forward propagation to a set of NN weights and biases.
#
# Args:
#     input: A vector(numeric) representing one row of the input.
#     weights: A list of weights built by casl_nn_init_weights.
#     sigma: The activation function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_forward_prop <- function(input, weights, sigma){
len <- length(weights)
z <- vector("list", len)
a <- vector("list", len)
for (i in seq_len(len)){
a_i1 <- if(i == 1) input else a[[i - 1]]
z[[i]] <- weights[[i]]$w %*% a_i1 + weights[[i]]$b
a[[i]] <- if (i != len) sigma(z[[i]]) else z[[i]]
}
list(z = z, a = a)
}
# casl_nn_backward_prop API
# Apply backward propagation algorithm.
#
# Args:
#     input: A vector(numeric) representing one row of the input.
#     output: A vector(numeric) representing one row of the response.
#     weights: A list created by casl_nn_init_weights.
#     forwardpp_obj: Output of the function casl_nn_forward_prop.
#     sigma_p: Derivative of the activation function.
#     f_p: Derivative of the loss function.
#
# Returns:
#     A list containing the new weighted responses (z) and
#     activations (a).
casl_nn_backward_prop <- function(input, output, weights, forwardpp_obj, sigma_p, f_p){
z <- forwardpp_obj$z
a <- forwardpp_obj$a
len <- length(weights)
grad_z <- vector("list", len)
grad_w <- vector("list", len)
for (i in rev(seq_len(len))){
if (i == len) {
grad_z[[i]] <- f_p(output, a[[i]])
}
else {
grad_z[[i]] <- (t(weights[[i + 1]]$w) %*% grad_z[[i + 1]]) * sigma_p(z[[i]])
}
a_j1 <- if(i == 1) input else a[[i - 1]]
grad_w[[i]] <- grad_z[[i]] %*% t(a_j1)
}
list(grad_z = grad_z, grad_w = grad_w)
}
# casl_nn_sgd_mad API
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A data(numeric) matrix.
#     y: A vector(numeric) of responses.
#     layers_sizes: A vector(numeric) showing the sizes of layers in NN.
#     epochs: Integer number of epochs to computer.
#     lr: Learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd_mad <- function(X, y, layer_sizes, epochs, lr, weights=NULL){
if (is.null(weights)){
weights <- casl_nn_init_weights(layer_sizes)
}
# for every individual, update the weights and repeat the procedure over all individuals.
for (epoch in seq_len(epochs)){
# propergations
for (i in seq_len(nrow(X))){
# excute forward propergation
forwardpp_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
# excute backward propergation
backwardpp_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
forwardpp_obj, casl_util_ReLU_derivative, casl_util_mad_p)
# update weights matrics
for (j in seq_along(weights)){
weights[[j]]$b <- weights[[j]]$b -lr * backwardpp_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -lr * backwardpp_obj$grad_w[[j]]
}
}
}
weights
}
# casl_nn_predict function
# Predict values from a training neural network.
#
# Args:
#     weights: List of weights describing the neural network.
#     X_test: A numeric data matrix for the predictions.
#
# Returns:
#     A matrix of predicted values.
casl_nn_predict <- function(weights, X_test){
p <- length(weights[[length(weights)]]$b)
y_hat <- matrix(0, ncol = p, nrow = nrow(X_test))
# excute prediction by forward propergation
for (i in seq_len(nrow(X_test))){
a <- casl_nn_forward_prop(X_test[i,], weights, casl_util_ReLU)$a
y_hat[i, ] <- a[[length(a)]]
}
y_hat
}
library(tidyverse)
#Simulate data using MAD
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
yn <- X[, 1, drop = FALSE]^2 + rnorm(1000, sd = 0.1)
# change some yn to be very large (outliers)
ind <- sample(seq_along(yn), 100)
yn[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))
# train and update weights
weights <- casl_nn_sgd_mad(X, yn, layer_sizes = c(1, 25, 1), epochs = 15, lr = 0.01)
# predict with trained weights
y_pred <- casl_nn_predict(weights, X)
# visualiza the true value and predicted value
d <- tibble(x = as.vector(X), y_pred = as.vector(y_pred),
y = X[, 1]^2, yn = as.vector(yn))
# plot  result with mean absolute deviation
ggplot(d) +
geom_point(aes(x = x, y = yn)) +
geom_point(aes(x = x, y = y_pred, color = "pink", alpha = 1.5)) +
labs(x = "x", y = "true/predicted", title = "True and Predicted Values with Mean Absolute Deviance (MAD)") +
theme(legend.position="None") + labs(subtitle="Pink: Predicted; Black: True") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
# Derivative of the mean squared error (MSE) function.
#
# Args:
#     y: A numeric vector of responses.
#     pred: A numeric vector of predicted responses.
#
# Returns:
#     Returned current derivative the MSE function.
casl_util_mse_derivative <- function(y, pred){
2 * (pred - y)
}
# Apply stochastic gradient descent (SGD) to estimate NN.
#
# Args:
#     X: A numeric data matrix.
#     y: A numeric vector of responses.
#     sizes: A numeric vector giving the sizes of layers in
#            the neural network.
#     epochs: Integer number of epochs to computer.
#     lr: Learning rate.
#     weights: Optional list of starting weights.
#
# Returns:
#     A list containing the trained weights for the network.
casl_nn_sgd_mse <- function(X, y, layer_sizes, epochs, lr, weights=NULL){
if (is.null(weights)){
weights <- casl_nn_init_weights(layer_sizes)
}
## for every individual, update the weights; repeat the procedure over all individuals.
for (epoch in seq_len(epochs)){
# propergations
for (i in seq_len(nrow(X))){
# forward propergations
forwardpp_obj <- casl_nn_forward_prop(X[i,], weights, casl_util_ReLU)
# backward propergations
backwardpp_obj <- casl_nn_backward_prop(X[i,], y[i,], weights,
forwardpp_obj, casl_util_ReLU_derivative, casl_util_mse_derivative)
# update weights
for (j in seq_along(weights)){
weights[[j]]$b <- weights[[j]]$b -lr * backwardpp_obj$grad_z[[j]]
weights[[j]]$w <- weights[[j]]$w -lr * backwardpp_obj$grad_w[[j]]
}
}
}
weights
}
#Simulate data processing for MSE
X <- matrix(runif(1000, min = -1, max = 1), ncol = 1)
yn <- X[,1,drop = FALSE]^ 2 + rnorm(1000, sd = 0.1)
# change some yn to be very large (outliers)
ind <- sample(seq_along(yn), 100)
yn[sort(ind)] <- c(runif(50, -10, -5), runif(50, 5, 10))
# train and update weights
weights <- casl_nn_sgd_mse(X, yn, layer_sizes = c(1, 25, 1), epochs = 15, lr = 0.01)
# predict with trained weights
y_pred <- casl_nn_predict(weights, X)
# visualiza the true value and predicted value
d <- tibble(x = as.vector(X), y_pred = as.vector(y_pred),
y = X[,1]^2, yn = as.vector(yn))
# plot results with mean square error
ggplot(d) +
geom_point(aes(x = x, y = yn)) +
geom_point(aes(x = x, y = y_pred, color = "pink", alpha = 1.5)) +
labs(x = "x", y = "true/predicted", title = "True and Predicted Values with Mean Squared Error (MSE)") +
theme(legend.position="None") + labs(subtitle="Pink: Predicted; Black: True") +
theme(plot.title = element_text(hjust = 0.5), plot.subtitle = element_text(hjust = 0.5))
x_train <- emnist$dataset[[1]][[1]]
load(file='/Users/wentinggao/Downloads/emnist.RData')
x_train <- emnist$dataset[[1]][[1]]
x_test <- emnist$dataset[[1]][[2]]
library(keras)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
x_train <- emnist$dataset[[1]][[1]]ï¼255
x_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim=c(nrow(X_train), 28, 28))
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim=c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1][2]])-1
y_train <- as.vector(emnist$dataset[[1][[2]])-1
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, 26)
y_train <- to_categorical(y_train, num_classes=26)
num_classes = 26
y_train <- to_categorical(y_train, num_classes)
num_classes <- 26
y_train <- to_categorical(y_train, num_classes)
x_test <- array(dim=c(nrow(X_test), 28, 28))
X_test <- emnist$dataset[[1]][[2]]
x_test <- array(dim=c(nrow(X_test), 28, 28))
x_test <- array(dim=c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
at('x_train_shape:', dim(x_train), '\n')
cat('x_train_shape:', dim(x_train), '\n')
cat(nrow(x_train), 'train samples\n')
cat(nrow(x_test), 'test samples\n')
x_test <- array(dim=c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
for (i in 1:nrow(X_test)) {
x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))
rm()\
rm()
load(file='/Users/wentinggao/Downloads/emnist.RData')
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim=c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, num_classes = 26)
load("emnist.Rdata")
load("/Users/wentinggao/Downloads/emnist.Rdata")
# train set
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
y_train <- to_categorical(y_train, num_classes = 26)
# test set
X_test <- emnist$dataset[[2]][[1]]/255
x_test <- array(dim = c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
y_test <- as.vector(emnist$dataset[[2]][[2]])-1
y_test <- to_categorical(y_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
library(keras)
load("/Users/wentinggao/Downloads/emnist.Rdata")
# train set
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
rm()
library(keras)
load("/Users/wentinggao/Downloads/emnist.Rdata")
x_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(x_train), 28, 28))
for (i in 1:nrow(x_train)) {
x_train[i,,] <- matrix(x_train[i,], nrow=28, ncol=28)
}
y_train <- as.vector(emnist$dataset[[1]][[2]])-1
X_train <- emnist$dataset[[1]][[1]]/255
X_test <- emnist$dataset[[2]][[1]]/255
y_test <- as.vector(emnist$dataset[[2]][[2]])-1
rm(emnist)
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
rm(X_train)
y_train <- to_categorical(y_train, num_classes = 26)
num_classes = 26
y_train <- to_categorical(y_train, num_classes )
x_test <- array(dim = c(nrow(X_test), 28, 28))
for (i in 1:nrow(X_test)) {
x_test[i,,] <- matrix(X_test[i,], nrow=28, ncol=28)
}
rm(X_test)
y_test <- to_categorical(y_test)
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
model <- keras_model_sequential()
model %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
input_shape = c(28, 28, 1),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_dropout(rate = 0.5) %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_conv_2d(filters = 32, kernel_size = c(5,5),
padding = "same") %>%
layer_activation(activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(5,5)) %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dense(units = 128) %>%
layer_activation(activation = "relu") %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 26) %>%
layer_activation(activation = "softmax")
summary(model)
# Compile model
model %>% compile(loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy'))
# Train model with an early stop
model %>% fit(
x_train, y_train,
validation_data = list(x_test,y_test),
epochs = 15,
callbacks=list(callback_early_stopping(patience=2, monitor='val_acc'))
)
load("/Users/wentinggao/Downloads/emnist.Rdata")
# train set
X_train <- emnist$dataset[[1]][[1]]/255
x_train <- array(dim = c(nrow(X_train), 28, 28))
for (i in 1:nrow(X_train)) {
x_train[i,,] <- matrix(X_train[i,], nrow=28, ncol=28)
}
